{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "import_+_accord_inter_annotateurs_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4181830af7b4ccbbee199a5a2f49e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a948efefd714b72911ae060b7c95b4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_108740df081d4b3590bb802a9324c7c2",
              "IPY_MODEL_de1a5d1774b7479ba0f40463949ae795"
            ]
          }
        },
        "9a948efefd714b72911ae060b7c95b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "108740df081d4b3590bb802a9324c7c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db29b464f267476d92923c0b11b0ec7c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f059d8fb2f6f4f068f07d443d0067e9d"
          }
        },
        "de1a5d1774b7479ba0f40463949ae795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_655556acf7314708ad2c11c3c661bdf7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 800kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dfda93994c9d4ff28bf258730cbb280a"
          }
        },
        "db29b464f267476d92923c0b11b0ec7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f059d8fb2f6f4f068f07d443d0067e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "655556acf7314708ad2c11c3c661bdf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dfda93994c9d4ff28bf258730cbb280a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0eb106059614cf8a8c7d46c54d0ec10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f8de44df6d34434a722af8988cb30a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ca6fdf2aee564a068052638f40b45d47",
              "IPY_MODEL_223fdbe22cda41e6ad16f77cb9a444f4"
            ]
          }
        },
        "2f8de44df6d34434a722af8988cb30a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca6fdf2aee564a068052638f40b45d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_57aa8abf7aa24f22a2ffc3f9f526e20f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e3a908e14db405fbd2441f23fd2f0bf"
          }
        },
        "223fdbe22cda41e6ad16f77cb9a444f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ad55337873d046d5ad73c888e2beaadd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 508/508 [00:00&lt;00:00, 1.93kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bcf25dff25949ff843c92ad801838c1"
          }
        },
        "57aa8abf7aa24f22a2ffc3f9f526e20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e3a908e14db405fbd2441f23fd2f0bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad55337873d046d5ad73c888e2beaadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bcf25dff25949ff843c92ad801838c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05f67cfcd1464ab5ba522e31b600b617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2068460c33564621b441f57d8a448444",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c6f998612de4f20b649e47a023bbdfc",
              "IPY_MODEL_2324711fe1164aaa9b2eac8f38b54f5a"
            ]
          }
        },
        "2068460c33564621b441f57d8a448444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c6f998612de4f20b649e47a023bbdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_64e0a9e5042645c6812194d6e5ded852",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 445032417,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 445032417,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d89685271749689db2160f7809ecfc"
          }
        },
        "2324711fe1164aaa9b2eac8f38b54f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fa8c2d3d04c54dc5a8a7746462b5bbde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 445M/445M [00:07&lt;00:00, 55.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60b75776b67e4b4891b82874dcddbddc"
          }
        },
        "64e0a9e5042645c6812194d6e5ded852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d89685271749689db2160f7809ecfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa8c2d3d04c54dc5a8a7746462b5bbde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60b75776b67e4b4891b82874dcddbddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oriented-division",
        "outputId": "c001cdcb-8ce9-481e-fe28-eb14f19706a9"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "id": "oriented-division",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "falling-graham"
      },
      "source": [
        "df1  = pd.read_json('/content/drive/My Drive/Données extraites de Doccano/file(9).json', lines=True)\r\n",
        "df2  = pd.read_json('/content/drive/My Drive/Données extraites de Doccano/file(3).json', lines=True)\r\n",
        "df3  = pd.read_json('/content/drive/My Drive/Données extraites de Doccano/file(4).json', lines=True)"
      ],
      "id": "falling-graham",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entitled-cardiff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "628681d4-fd07-48dd-b8f1-7fb025a48944"
      },
      "source": [
        "df1.head(20)"
      ],
      "id": "entitled-cardiff",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotations</th>\n",
              "      <th>meta</th>\n",
              "      <th>annotation_approver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33334</td>\n",
              "      <td>HOLLAND, Pa. -- The Tea Party movement ignited...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 3065, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33335</td>\n",
              "      <td>WASHINGTON, Oct. 12 -- President Bush delivere...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 23, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33336</td>\n",
              "      <td>AS oil and gasoline prices rise, political pre...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33337</td>\n",
              "      <td>Caucusgoers, uniquely, will declare their loya...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 165, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33338</td>\n",
              "      <td>To the Editor:    Re ''Giuliani Says a Democra...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 547, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33339</td>\n",
              "      <td>CARACAS, Venezuela, Nov. 29 -- Opponents of Pr...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 31, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>33340</td>\n",
              "      <td>WASHINGTON -- Leading Democrats on the House J...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 284, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33341</td>\n",
              "      <td>WASHINGTON -- George W. Bush has just placed h...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 1864, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33342</td>\n",
              "      <td>WASHINGTON, March 26 -- The World Trade Organi...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 24, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33343</td>\n",
              "      <td>If you listened to the Republican candidates t...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 99, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33344</td>\n",
              "      <td>A bill banning abortions in Albuquerque after ...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>33345</td>\n",
              "      <td>WASHINGTON -- Senator Barack Obama informed tw...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 14, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>33346</td>\n",
              "      <td>WASHINGTON, Feb. 23 -- The Energy Department s...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 629, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>33347</td>\n",
              "      <td>Most contributors to political campaigns are p...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 418, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>33348</td>\n",
              "      <td>WASHINGTON -- Government investigators said Fr...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 14, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>33349</td>\n",
              "      <td>WASHINGTON, Aug. 2 -- Federal officials moved ...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 22, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>33350</td>\n",
              "      <td>Mayor Michael R. Bloomberg announced yesterday...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 253, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>33351</td>\n",
              "      <td>Like many areas around the country, Washington...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>33352</td>\n",
              "      <td>Yes, the American military still uses bayonets...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 1755, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>33353</td>\n",
              "      <td>Washington Tax Break Urged for Firms That Keep...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 722, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... annotation_approver\n",
              "0   33334  ...                 NaN\n",
              "1   33335  ...                 NaN\n",
              "2   33336  ...                 NaN\n",
              "3   33337  ...                 NaN\n",
              "4   33338  ...                 NaN\n",
              "5   33339  ...                 NaN\n",
              "6   33340  ...                 NaN\n",
              "7   33341  ...                 NaN\n",
              "8   33342  ...                 NaN\n",
              "9   33343  ...                 NaN\n",
              "10  33344  ...                 NaN\n",
              "11  33345  ...                 NaN\n",
              "12  33346  ...                 NaN\n",
              "13  33347  ...                 NaN\n",
              "14  33348  ...                 NaN\n",
              "15  33349  ...                 NaN\n",
              "16  33350  ...                 NaN\n",
              "17  33351  ...                 NaN\n",
              "18  33352  ...                 NaN\n",
              "19  33353  ...                 NaN\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6ZqfXMO1lbY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "4ee7367c-c4d0-41a0-9e7a-91d87552ee0f"
      },
      "source": [
        "df2.head()"
      ],
      "id": "T6ZqfXMO1lbY",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotations</th>\n",
              "      <th>meta</th>\n",
              "      <th>annotation_approver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33364</td>\n",
              "      <td>HOLLAND, Pa. -- The Tea Party movement ignited...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 1039, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33365</td>\n",
              "      <td>WASHINGTON, Oct. 12 -- President Bush delivere...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33366</td>\n",
              "      <td>AS oil and gasoline prices rise, political pre...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33367</td>\n",
              "      <td>Caucusgoers, uniquely, will declare their loya...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 297, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33368</td>\n",
              "      <td>To the Editor:    Re ''Giuliani Says a Democra...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... annotation_approver\n",
              "0  33364  ...                 NaN\n",
              "1  33365  ...                 NaN\n",
              "2  33366  ...                 NaN\n",
              "3  33367  ...                 NaN\n",
              "4  33368  ...                 NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conservative-summary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a1dbd0-9abd-47a0-e3ea-79fa584cb89d"
      },
      "source": [
        "df1.annotations.iloc[0]"
      ],
      "id": "conservative-summary",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'end_offset': 3260, 'label': 449, 'start_offset': 3065, 'user': 51},\n",
              " {'end_offset': 4651, 'label': 450, 'start_offset': 4553, 'user': 51},\n",
              " {'end_offset': 173, 'label': 450, 'start_offset': 94, 'user': 50},\n",
              " {'end_offset': 452, 'label': 450, 'start_offset': 174, 'user': 50},\n",
              " {'end_offset': 1388, 'label': 450, 'start_offset': 1178, 'user': 50},\n",
              " {'end_offset': 3577, 'label': 449, 'start_offset': 3506, 'user': 50},\n",
              " {'end_offset': 4040, 'label': 450, 'start_offset': 3892, 'user': 50},\n",
              " {'end_offset': 4182, 'label': 450, 'start_offset': 4040, 'user': 50},\n",
              " {'end_offset': 6967, 'label': 449, 'start_offset': 6759, 'user': 50},\n",
              " {'end_offset': 7447, 'label': 450, 'start_offset': 7209, 'user': 50},\n",
              " {'end_offset': 1968, 'label': 450, 'start_offset': 1818, 'user': 50},\n",
              " {'end_offset': 2121, 'label': 450, 'start_offset': 1968, 'user': 50},\n",
              " {'end_offset': 3259, 'label': 450, 'start_offset': 3064, 'user': 50},\n",
              " {'end_offset': 3505, 'label': 450, 'start_offset': 3479, 'user': 50},\n",
              " {'end_offset': 7692, 'label': 450, 'start_offset': 7447, 'user': 50},\n",
              " {'end_offset': 7209, 'label': 450, 'start_offset': 7071, 'user': 50},\n",
              " {'end_offset': 7071, 'label': 450, 'start_offset': 6967, 'user': 50},\n",
              " {'end_offset': 6629, 'label': 450, 'start_offset': 6513, 'user': 50},\n",
              " {'end_offset': 6265, 'label': 450, 'start_offset': 6087, 'user': 50},\n",
              " {'end_offset': 2121, 'label': 450, 'start_offset': 1969, 'user': 53},\n",
              " {'end_offset': 3891, 'label': 449, 'start_offset': 3578, 'user': 53},\n",
              " {'end_offset': 173, 'label': 450, 'start_offset': 95, 'user': 53},\n",
              " {'end_offset': 2407, 'label': 450, 'start_offset': 2228, 'user': 53},\n",
              " {'end_offset': 2677, 'label': 450, 'start_offset': 2407, 'user': 53},\n",
              " {'end_offset': 4437, 'label': 450, 'start_offset': 4342, 'user': 53},\n",
              " {'end_offset': 4552, 'label': 450, 'start_offset': 4438, 'user': 53},\n",
              " {'end_offset': 6266, 'label': 450, 'start_offset': 6086, 'user': 53},\n",
              " {'end_offset': 6628, 'label': 450, 'start_offset': 6513, 'user': 53},\n",
              " {'end_offset': 7071, 'label': 450, 'start_offset': 6967, 'user': 53},\n",
              " {'end_offset': 7209, 'label': 450, 'start_offset': 7071, 'user': 53},\n",
              " {'end_offset': 7446, 'label': 450, 'start_offset': 7209, 'user': 53},\n",
              " {'end_offset': 7692, 'label': 450, 'start_offset': 7446, 'user': 53},\n",
              " {'end_offset': 7787, 'label': 449, 'start_offset': 7693, 'user': 53},\n",
              " {'end_offset': 7934, 'label': 450, 'start_offset': 7787, 'user': 53},\n",
              " {'end_offset': 717, 'label': 450, 'start_offset': 452, 'user': 50},\n",
              " {'end_offset': 452, 'label': 450, 'start_offset': 173, 'user': 53},\n",
              " {'end_offset': 718, 'label': 450, 'start_offset': 452, 'user': 53},\n",
              " {'end_offset': 1178, 'label': 450, 'start_offset': 894, 'user': 53},\n",
              " {'end_offset': 1817, 'label': 450, 'start_offset': 1547, 'user': 53},\n",
              " {'end_offset': 1969, 'label': 450, 'start_offset': 1818, 'user': 53},\n",
              " {'end_offset': 4257, 'label': 450, 'start_offset': 4183, 'user': 53},\n",
              " {'end_offset': 4857, 'label': 450, 'start_offset': 4552, 'user': 53},\n",
              " {'end_offset': 5163, 'label': 450, 'start_offset': 5140, 'user': 53},\n",
              " {'end_offset': 5410, 'label': 450, 'start_offset': 5163, 'user': 53},\n",
              " {'end_offset': 5504, 'label': 450, 'start_offset': 5410, 'user': 53},\n",
              " {'end_offset': 5585, 'label': 450, 'start_offset': 5504, 'user': 53},\n",
              " {'end_offset': 1462, 'label': 450, 'start_offset': 1388, 'user': 53},\n",
              " {'end_offset': 6086, 'label': 450, 'start_offset': 5950, 'user': 53},\n",
              " {'end_offset': 94, 'label': 450, 'start_offset': 16, 'user': 50},\n",
              " {'end_offset': 95, 'label': 450, 'start_offset': 16, 'user': 53},\n",
              " {'end_offset': 1178, 'label': 450, 'start_offset': 894, 'user': 50},\n",
              " {'end_offset': 1388, 'label': 450, 'start_offset': 1178, 'user': 53},\n",
              " {'end_offset': 1463, 'label': 450, 'start_offset': 1389, 'user': 50},\n",
              " {'end_offset': 1818, 'label': 450, 'start_offset': 1547, 'user': 50},\n",
              " {'end_offset': 2408, 'label': 450, 'start_offset': 2228, 'user': 50},\n",
              " {'end_offset': 2677, 'label': 450, 'start_offset': 2408, 'user': 50},\n",
              " {'end_offset': 3064, 'label': 450, 'start_offset': 2677, 'user': 50},\n",
              " {'end_offset': 3065, 'label': 450, 'start_offset': 2677, 'user': 53},\n",
              " {'end_offset': 3260, 'label': 450, 'start_offset': 3065, 'user': 53},\n",
              " {'end_offset': 3506, 'label': 450, 'start_offset': 3479, 'user': 53},\n",
              " {'end_offset': 3578, 'label': 449, 'start_offset': 3506, 'user': 53},\n",
              " {'end_offset': 3892, 'label': 449, 'start_offset': 3577, 'user': 50},\n",
              " {'end_offset': 4040, 'label': 450, 'start_offset': 3891, 'user': 53},\n",
              " {'end_offset': 4183, 'label': 450, 'start_offset': 4041, 'user': 53},\n",
              " {'end_offset': 7935, 'label': 450, 'start_offset': 7789, 'user': 50},\n",
              " {'end_offset': 7789, 'label': 449, 'start_offset': 7693, 'user': 50},\n",
              " {'end_offset': 6967, 'label': 449, 'start_offset': 6760, 'user': 53},\n",
              " {'end_offset': 6087, 'label': 450, 'start_offset': 5950, 'user': 50},\n",
              " {'end_offset': 5586, 'label': 450, 'start_offset': 5504, 'user': 50},\n",
              " {'end_offset': 5504, 'label': 450, 'start_offset': 5411, 'user': 50},\n",
              " {'end_offset': 5411, 'label': 450, 'start_offset': 5164, 'user': 50},\n",
              " {'end_offset': 5164, 'label': 450, 'start_offset': 5140, 'user': 50},\n",
              " {'end_offset': 4856, 'label': 450, 'start_offset': 4553, 'user': 50},\n",
              " {'end_offset': 4942, 'label': 450, 'start_offset': 4856, 'user': 50},\n",
              " {'end_offset': 5049, 'label': 450, 'start_offset': 4942, 'user': 50},\n",
              " {'end_offset': 5140, 'label': 450, 'start_offset': 5049, 'user': 50},\n",
              " {'end_offset': 5140, 'label': 450, 'start_offset': 5050, 'user': 53},\n",
              " {'end_offset': 5050, 'label': 450, 'start_offset': 4941, 'user': 53},\n",
              " {'end_offset': 4941, 'label': 450, 'start_offset': 4857, 'user': 53},\n",
              " {'end_offset': 4553, 'label': 450, 'start_offset': 4438, 'user': 50},\n",
              " {'end_offset': 4436, 'label': 450, 'start_offset': 4342, 'user': 50},\n",
              " {'end_offset': 4256, 'label': 450, 'start_offset': 4183, 'user': 50}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "composite-rating"
      },
      "source": [
        "#fonction qui permet ensuite d'obtenir les annotations pour chaque texte de chaque annotateur (1 annotateur = 1 colonne)\n",
        "def simplify_annotations(x):\n",
        "  users = {}\n",
        "  for dic in x:\n",
        "    if str(dic['user']) not in users.keys():\n",
        "    #print(users.keys())\n",
        "      users[str(dic['user'])] = []\n",
        "    users[str(dic['user'])].append([dic['start_offset'], dic['end_offset'], dic['label']])\n",
        "  return users"
      ],
      "id": "composite-rating",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlimited-terry"
      },
      "source": [
        "df1['simplified'] = df1.annotations.apply(simplify_annotations)\r\n",
        "df2['simplified'] = df2.annotations.apply(simplify_annotations)\r\n",
        "df3['simplified'] = df3.annotations.apply(simplify_annotations)"
      ],
      "id": "unlimited-terry",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "educated-slovakia"
      },
      "source": [
        "# TODO : condition sur les numéros des annotateurs : bien vérifier qu'on n'en oublie pas\n",
        "\n",
        "#Pour df1, il y a: dict_keys([]) ; dict_keys(['50']) ; dict_keys(['53', '50']) ; dict_keys(['50', '53']) ; dict_keys(['53']) ;\n",
        "# dict_keys(['51', '50', '53'])\n",
        "df1['annotateur_50_1'] = df1.simplified.apply(lambda x: x['50'] if '50' in x.keys() else []) \n",
        "df1['annotateur_53_1'] = df1.simplified.apply(lambda x: x['53'] if '53' in x.keys() else [])\n",
        "\n",
        "#Pour df2, il y a: dict_keys([]) ; dict_keys(['51']) ; dict_keys(['52']) ; dict_keys(['51', '52']) ; \n",
        "df2['annotateur_51_2'] = df2.simplified.apply(lambda x: x['51'] if '51' in x.keys() else [])\n",
        "df2['annotateur_52_2'] = df2.simplified.apply(lambda x: x['52'] if '52' in x.keys() else [])\n",
        "\n",
        "#Pour df3, il y a: dict_keys([]) ; dict_keys(['51']) ; dict_keys(['52']) ; dict_keys(['52', '51']) ; dict_keys(['51', '52'])\n",
        "df3['annotateur_51_3'] = df3.simplified.apply(lambda x: x['51'] if '51' in x.keys() else [])\n",
        "df3['annotateur_52_3'] = df3.simplified.apply(lambda x: x['52'] if '52' in x.keys() else [])\n"
      ],
      "id": "educated-slovakia",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confident-blond",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "776bb9d2-adbd-4c1c-faf6-eb56db5bdd3b"
      },
      "source": [
        "df1.head(20)"
      ],
      "id": "confident-blond",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotations</th>\n",
              "      <th>meta</th>\n",
              "      <th>annotation_approver</th>\n",
              "      <th>simplified</th>\n",
              "      <th>annotateur_50_1</th>\n",
              "      <th>annotateur_53_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33334</td>\n",
              "      <td>HOLLAND, Pa. -- The Tea Party movement ignited...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 3065, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[3065, 3260, 449], [4553, 4651, 450]],...</td>\n",
              "      <td>[[94, 173, 450], [174, 452, 450], [1178, 1388,...</td>\n",
              "      <td>[[1969, 2121, 450], [3578, 3891, 449], [95, 17...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33335</td>\n",
              "      <td>WASHINGTON, Oct. 12 -- President Bush delivere...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 23, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[23, 212, 450], [212, 359, 450], [359,...</td>\n",
              "      <td>[[23, 212, 450], [212, 359, 450], [359, 441, 4...</td>\n",
              "      <td>[[1018, 1135, 449], [214, 358, 450], [1954, 21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33336</td>\n",
              "      <td>AS oil and gasoline prices rise, political pre...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[0, 181, 450], [879, 997, 450], [1144,...</td>\n",
              "      <td>[[0, 181, 450], [879, 997, 450], [1144, 1467, ...</td>\n",
              "      <td>[[0, 180, 450], [470, 655, 450], [879, 997, 45...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33337</td>\n",
              "      <td>Caucusgoers, uniquely, will declare their loya...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 165, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[165, 353, 450], [353, 391, 450], [391...</td>\n",
              "      <td>[[165, 353, 450], [353, 391, 450], [391, 417, ...</td>\n",
              "      <td>[[392, 417, 450], [165, 352, 450], [418, 472, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33338</td>\n",
              "      <td>To the Editor:    Re ''Giuliani Says a Democra...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 547, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[547, 627, 449]], '50': [[547, 627, 44...</td>\n",
              "      <td>[[547, 627, 449]]</td>\n",
              "      <td>[[547, 627, 449]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33339</td>\n",
              "      <td>CARACAS, Venezuela, Nov. 29 -- Opponents of Pr...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 31, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[31, 194, 450], [196, 288, 449], [289,...</td>\n",
              "      <td>[[31, 194, 450], [196, 288, 449], [289, 387, 4...</td>\n",
              "      <td>[[31, 194, 450], [195, 289, 449], [389, 569, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>33340</td>\n",
              "      <td>WASHINGTON -- Leading Democrats on the House J...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 284, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[284, 585, 450], [843, 924, 450], [924...</td>\n",
              "      <td>[[284, 585, 450], [843, 924, 450], [924, 1201,...</td>\n",
              "      <td>[[14, 284, 449], [2442, 2651, 450], [2842, 300...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33341</td>\n",
              "      <td>WASHINGTON -- George W. Bush has just placed h...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 1864, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[1864, 1977, 450], [2746, 2884, 450], ...</td>\n",
              "      <td>[[1864, 1977, 450], [2746, 2884, 450], [2940, ...</td>\n",
              "      <td>[[14, 88, 449], [1721, 1862, 450], [2449, 2621...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33342</td>\n",
              "      <td>WASHINGTON, March 26 -- The World Trade Organi...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 24, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[24, 137, 450], [138, 193, 450], [479,...</td>\n",
              "      <td>[[24, 137, 450], [138, 193, 450], [479, 682, 4...</td>\n",
              "      <td>[[140, 194, 450], [843, 984, 450], [984, 1110,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33343</td>\n",
              "      <td>If you listened to the Republican candidates t...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 99, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[99, 199, 449], [361, 451, 450], [1306...</td>\n",
              "      <td>[[99, 199, 449], [361, 451, 450], [1306, 1449,...</td>\n",
              "      <td>[[99, 199, 449], [200, 361, 449], [451, 621, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33344</td>\n",
              "      <td>A bill banning abortions in Albuquerque after ...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[0, 98, 450], [99, 213, 450], [334, 41...</td>\n",
              "      <td>[[0, 98, 450], [99, 213, 450], [334, 411, 450]...</td>\n",
              "      <td>[[0, 97, 450], [99, 213, 450], [214, 333, 450]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>33345</td>\n",
              "      <td>WASHINGTON -- Senator Barack Obama informed tw...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 14, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[14, 273, 449], [273, 338, 450], [869,...</td>\n",
              "      <td>[[13, 273, 449], [273, 338, 450], [338, 509, 4...</td>\n",
              "      <td>[[14, 273, 449], [273, 338, 450], [869, 1020, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>33346</td>\n",
              "      <td>WASHINGTON, Feb. 23 -- The Energy Department s...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 629, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[629, 764, 450], [368, 628, 450]], '53...</td>\n",
              "      <td>[[629, 764, 450], [368, 628, 450]]</td>\n",
              "      <td>[[207, 363, 450], [366, 629, 450], [369, 628, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>33347</td>\n",
              "      <td>Most contributors to political campaigns are p...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 418, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[418, 588, 450], [686, 898, 449], [256...</td>\n",
              "      <td>[[0, 149, 449], [149, 255, 449], [256, 354, 44...</td>\n",
              "      <td>[[418, 588, 450], [686, 898, 449], [256, 353, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>33348</td>\n",
              "      <td>WASHINGTON -- Government investigators said Fr...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 14, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[14, 304, 449], [1252, 1542, 449], [15...</td>\n",
              "      <td>[[14, 304, 449], [500, 608, 450], [1252, 1542,...</td>\n",
              "      <td>[[14, 304, 449], [1252, 1542, 449], [1542, 161...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>33349</td>\n",
              "      <td>WASHINGTON, Aug. 2 -- Federal officials moved ...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 22, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[22, 351, 450], [2234, 2340, 450], [34...</td>\n",
              "      <td>[[22, 350, 450], [795, 1120, 450], [1121, 1394...</td>\n",
              "      <td>[[22, 351, 450], [2234, 2340, 450], [3402, 345...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>33350</td>\n",
              "      <td>Mayor Michael R. Bloomberg announced yesterday...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 253, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[253, 463, 450], [0, 252, 449]], '50':...</td>\n",
              "      <td>[[0, 252, 449], [253, 464, 450]]</td>\n",
              "      <td>[[253, 463, 450], [0, 252, 449]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>33351</td>\n",
              "      <td>Like many areas around the country, Washington...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[0, 131, 450], [943, 1127, 450], [145,...</td>\n",
              "      <td>[[146, 394, 449], [397, 633, 450], [707, 942, ...</td>\n",
              "      <td>[[0, 131, 450], [943, 1127, 450], [145, 394, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>33352</td>\n",
              "      <td>Yes, the American military still uses bayonets...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 1755, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[1755, 1876, 450], [2063, 2162, 450], ...</td>\n",
              "      <td>[[87, 300, 450], [301, 437, 450], [689, 939, 4...</td>\n",
              "      <td>[[1755, 1876, 450], [2063, 2162, 450], [87, 30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>33353</td>\n",
              "      <td>Washington Tax Break Urged for Firms That Keep...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 722, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[722, 869, 450], [62, 354, 450], [506,...</td>\n",
              "      <td>[[507, 685, 449], [685, 721, 450], [869, 997, ...</td>\n",
              "      <td>[[722, 869, 450], [62, 354, 450], [506, 685, 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ...                                    annotateur_53_1\n",
              "0   33334  ...  [[1969, 2121, 450], [3578, 3891, 449], [95, 17...\n",
              "1   33335  ...  [[1018, 1135, 449], [214, 358, 450], [1954, 21...\n",
              "2   33336  ...  [[0, 180, 450], [470, 655, 450], [879, 997, 45...\n",
              "3   33337  ...  [[392, 417, 450], [165, 352, 450], [418, 472, ...\n",
              "4   33338  ...                                  [[547, 627, 449]]\n",
              "5   33339  ...  [[31, 194, 450], [195, 289, 449], [389, 569, 4...\n",
              "6   33340  ...  [[14, 284, 449], [2442, 2651, 450], [2842, 300...\n",
              "7   33341  ...  [[14, 88, 449], [1721, 1862, 450], [2449, 2621...\n",
              "8   33342  ...  [[140, 194, 450], [843, 984, 450], [984, 1110,...\n",
              "9   33343  ...  [[99, 199, 449], [200, 361, 449], [451, 621, 4...\n",
              "10  33344  ...  [[0, 97, 450], [99, 213, 450], [214, 333, 450]...\n",
              "11  33345  ...  [[14, 273, 449], [273, 338, 450], [869, 1020, ...\n",
              "12  33346  ...  [[207, 363, 450], [366, 629, 450], [369, 628, ...\n",
              "13  33347  ...  [[418, 588, 450], [686, 898, 449], [256, 353, ...\n",
              "14  33348  ...  [[14, 304, 449], [1252, 1542, 449], [1542, 161...\n",
              "15  33349  ...  [[22, 351, 450], [2234, 2340, 450], [3402, 345...\n",
              "16  33350  ...                   [[253, 463, 450], [0, 252, 449]]\n",
              "17  33351  ...  [[0, 131, 450], [943, 1127, 450], [145, 394, 4...\n",
              "18  33352  ...  [[1755, 1876, 450], [2063, 2162, 450], [87, 30...\n",
              "19  33353  ...  [[722, 869, 450], [62, 354, 450], [506, 685, 4...\n",
              "\n",
              "[20 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "banned-penny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625d54c2-f05b-4319-bf88-e602322166af"
      },
      "source": [
        "!pip install krippendorff"
      ],
      "id": "banned-penny",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting krippendorff\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/8f/89453f18f3971677c4876405e3c511f84dbd86cfe4fa513b004a4d2b22cc/krippendorff-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from krippendorff) (1.19.5)\n",
            "Installing collected packages: krippendorff\n",
            "Successfully installed krippendorff-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broadband-abuse"
      },
      "source": [
        "import numpy as np\n",
        "import krippendorff"
      ],
      "id": "broadband-abuse",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sacred-steel"
      },
      "source": [
        "# Vecteur one-hot pour chaque annotation de chaque annotateur :\n",
        "\n",
        "def one_hot(x, n):\n",
        "    \"\"\"\n",
        "    n = longueur du texte, paramètre\n",
        "    x = colonne d'annotation à transformer en one-hot\n",
        "    \"\"\"\n",
        "    anno = x\n",
        "    ls = [0]*n\n",
        "    for annotation in anno:\n",
        "        ls[annotation[0]:annotation[1]] = [annotation[2]]*(annotation[1]-annotation[0])\n",
        "    return ls"
      ],
      "id": "sacred-steel",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f91NrrWD7CX_"
      },
      "source": [
        "Pour df1 : endo/exo"
      ],
      "id": "f91NrrWD7CX_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hindu-hands"
      },
      "source": [
        "#df1['one_hot_51_1'] = df1.apply(lambda x: one_hot(x.annotateur_51_1, len(x.text)), axis=1) à enlever\n",
        "df1['one_hot_50_1'] = df1.apply(lambda x: one_hot(x.annotateur_50_1, len(x.text)), axis=1)\n",
        "df1['one_hot_53_1'] = df1.apply(lambda x: one_hot(x.annotateur_53_1, len(x.text)), axis=1)"
      ],
      "id": "hindu-hands",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trained-impression",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e154665-f322-4a3a-cc2b-c44057bee8cf"
      },
      "source": [
        "df1.annotateur_50_1.iloc[0]"
      ],
      "id": "trained-impression",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[94, 173, 450],\n",
              " [174, 452, 450],\n",
              " [1178, 1388, 450],\n",
              " [3506, 3577, 449],\n",
              " [3892, 4040, 450],\n",
              " [4040, 4182, 450],\n",
              " [6759, 6967, 449],\n",
              " [7209, 7447, 450],\n",
              " [1818, 1968, 450],\n",
              " [1968, 2121, 450],\n",
              " [3064, 3259, 450],\n",
              " [3479, 3505, 450],\n",
              " [7447, 7692, 450],\n",
              " [7071, 7209, 450],\n",
              " [6967, 7071, 450],\n",
              " [6513, 6629, 450],\n",
              " [6087, 6265, 450],\n",
              " [452, 717, 450],\n",
              " [16, 94, 450],\n",
              " [894, 1178, 450],\n",
              " [1389, 1463, 450],\n",
              " [1547, 1818, 450],\n",
              " [2228, 2408, 450],\n",
              " [2408, 2677, 450],\n",
              " [2677, 3064, 450],\n",
              " [3577, 3892, 449],\n",
              " [7789, 7935, 450],\n",
              " [7693, 7789, 449],\n",
              " [5950, 6087, 450],\n",
              " [5504, 5586, 450],\n",
              " [5411, 5504, 450],\n",
              " [5164, 5411, 450],\n",
              " [5140, 5164, 450],\n",
              " [4553, 4856, 450],\n",
              " [4856, 4942, 450],\n",
              " [4942, 5049, 450],\n",
              " [5049, 5140, 450],\n",
              " [4438, 4553, 450],\n",
              " [4342, 4436, 450],\n",
              " [4183, 4256, 450]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ranging-queen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e2c839-5624-4cd3-aedf-c20ad61ffd48"
      },
      "source": [
        "print(df1['one_hot_50_1'].iloc[0])"
      ],
      "id": "ranging-queen",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 449, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "interpreted-belief",
        "outputId": "461bbea7-4a51-4957-87df-072f19ebbe2d"
      },
      "source": [
        "df1['text-wise-alpha'] = df1.apply(lambda x: krippendorff.alpha(reliability_data=[x.one_hot_50_1, x.one_hot_53_1]), axis=1)"
      ],
      "id": "interpreted-belief",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/krippendorff/krippendorff.py:273: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - (o * d).sum() / (e * d).sum()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ultimate-theta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "473ff2fd-9c79-45fd-df35-ed8794276527"
      },
      "source": [
        "df1.head(20)"
      ],
      "id": "ultimate-theta",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotations</th>\n",
              "      <th>meta</th>\n",
              "      <th>annotation_approver</th>\n",
              "      <th>simplified</th>\n",
              "      <th>annotateur_50_1</th>\n",
              "      <th>annotateur_53_1</th>\n",
              "      <th>one_hot_50_1</th>\n",
              "      <th>one_hot_53_1</th>\n",
              "      <th>text-wise-alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33334</td>\n",
              "      <td>HOLLAND, Pa. -- The Tea Party movement ignited...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 3065, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[3065, 3260, 449], [4553, 4651, 450]],...</td>\n",
              "      <td>[[94, 173, 450], [174, 452, 450], [1178, 1388,...</td>\n",
              "      <td>[[1969, 2121, 450], [3578, 3891, 449], [95, 17...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.993452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33335</td>\n",
              "      <td>WASHINGTON, Oct. 12 -- President Bush delivere...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 23, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[23, 212, 450], [212, 359, 450], [359,...</td>\n",
              "      <td>[[23, 212, 450], [212, 359, 450], [359, 441, 4...</td>\n",
              "      <td>[[1018, 1135, 449], [214, 358, 450], [1954, 21...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.989754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33336</td>\n",
              "      <td>AS oil and gasoline prices rise, political pre...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[0, 181, 450], [879, 997, 450], [1144,...</td>\n",
              "      <td>[[0, 181, 450], [879, 997, 450], [1144, 1467, ...</td>\n",
              "      <td>[[0, 180, 450], [470, 655, 450], [879, 997, 45...</td>\n",
              "      <td>[450, 450, 450, 450, 450, 450, 450, 450, 450, ...</td>\n",
              "      <td>[450, 450, 450, 450, 450, 450, 450, 450, 450, ...</td>\n",
              "      <td>0.997436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33337</td>\n",
              "      <td>Caucusgoers, uniquely, will declare their loya...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 165, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[165, 353, 450], [353, 391, 450], [391...</td>\n",
              "      <td>[[165, 353, 450], [353, 391, 450], [391, 417, ...</td>\n",
              "      <td>[[392, 417, 450], [165, 352, 450], [418, 472, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.993184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33338</td>\n",
              "      <td>To the Editor:    Re ''Giuliani Says a Democra...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 547, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[547, 627, 449]], '50': [[547, 627, 44...</td>\n",
              "      <td>[[547, 627, 449]]</td>\n",
              "      <td>[[547, 627, 449]]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33339</td>\n",
              "      <td>CARACAS, Venezuela, Nov. 29 -- Opponents of Pr...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 31, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[31, 194, 450], [196, 288, 449], [289,...</td>\n",
              "      <td>[[31, 194, 450], [196, 288, 449], [289, 387, 4...</td>\n",
              "      <td>[[31, 194, 450], [195, 289, 449], [389, 569, 4...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.930985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>33340</td>\n",
              "      <td>WASHINGTON -- Leading Democrats on the House J...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 284, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[284, 585, 450], [843, 924, 450], [924...</td>\n",
              "      <td>[[284, 585, 450], [843, 924, 450], [924, 1201,...</td>\n",
              "      <td>[[14, 284, 449], [2442, 2651, 450], [2842, 300...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>0.993963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33341</td>\n",
              "      <td>WASHINGTON -- George W. Bush has just placed h...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 1864, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[1864, 1977, 450], [2746, 2884, 450], ...</td>\n",
              "      <td>[[1864, 1977, 450], [2746, 2884, 450], [2940, ...</td>\n",
              "      <td>[[14, 88, 449], [1721, 1862, 450], [2449, 2621...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>0.761209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33342</td>\n",
              "      <td>WASHINGTON, March 26 -- The World Trade Organi...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 24, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[24, 137, 450], [138, 193, 450], [479,...</td>\n",
              "      <td>[[24, 137, 450], [138, 193, 450], [479, 682, 4...</td>\n",
              "      <td>[[140, 194, 450], [843, 984, 450], [984, 1110,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.874042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33343</td>\n",
              "      <td>If you listened to the Republican candidates t...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 99, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[99, 199, 449], [361, 451, 450], [1306...</td>\n",
              "      <td>[[99, 199, 449], [361, 451, 450], [1306, 1449,...</td>\n",
              "      <td>[[99, 199, 449], [200, 361, 449], [451, 621, 4...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.824292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33344</td>\n",
              "      <td>A bill banning abortions in Albuquerque after ...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[0, 98, 450], [99, 213, 450], [334, 41...</td>\n",
              "      <td>[[0, 98, 450], [99, 213, 450], [334, 411, 450]...</td>\n",
              "      <td>[[0, 97, 450], [99, 213, 450], [214, 333, 450]...</td>\n",
              "      <td>[450, 450, 450, 450, 450, 450, 450, 450, 450, ...</td>\n",
              "      <td>[450, 450, 450, 450, 450, 450, 450, 450, 450, ...</td>\n",
              "      <td>0.797818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>33345</td>\n",
              "      <td>WASHINGTON -- Senator Barack Obama informed tw...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 14, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[14, 273, 449], [273, 338, 450], [869,...</td>\n",
              "      <td>[[13, 273, 449], [273, 338, 450], [338, 509, 4...</td>\n",
              "      <td>[[14, 273, 449], [273, 338, 450], [869, 1020, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449, 4...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>0.667596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>33346</td>\n",
              "      <td>WASHINGTON, Feb. 23 -- The Energy Department s...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 629, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'50': [[629, 764, 450], [368, 628, 450]], '53...</td>\n",
              "      <td>[[629, 764, 450], [368, 628, 450]]</td>\n",
              "      <td>[[207, 363, 450], [366, 629, 450], [369, 628, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.559220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>33347</td>\n",
              "      <td>Most contributors to political campaigns are p...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 418, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[418, 588, 450], [686, 898, 449], [256...</td>\n",
              "      <td>[[0, 149, 449], [149, 255, 449], [256, 354, 44...</td>\n",
              "      <td>[[418, 588, 450], [686, 898, 449], [256, 353, ...</td>\n",
              "      <td>[449, 449, 449, 449, 449, 449, 449, 449, 449, ...</td>\n",
              "      <td>[449, 449, 449, 449, 449, 449, 449, 449, 449, ...</td>\n",
              "      <td>0.568560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>33348</td>\n",
              "      <td>WASHINGTON -- Government investigators said Fr...</td>\n",
              "      <td>[{'label': 449, 'start_offset': 14, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[14, 304, 449], [1252, 1542, 449], [15...</td>\n",
              "      <td>[[14, 304, 449], [500, 608, 450], [1252, 1542,...</td>\n",
              "      <td>[[14, 304, 449], [1252, 1542, 449], [1542, 161...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 449...</td>\n",
              "      <td>0.814418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>33349</td>\n",
              "      <td>WASHINGTON, Aug. 2 -- Federal officials moved ...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 22, 'end_offse...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[22, 351, 450], [2234, 2340, 450], [34...</td>\n",
              "      <td>[[22, 350, 450], [795, 1120, 450], [1121, 1394...</td>\n",
              "      <td>[[22, 351, 450], [2234, 2340, 450], [3402, 345...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.919943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>33350</td>\n",
              "      <td>Mayor Michael R. Bloomberg announced yesterday...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 253, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[253, 463, 450], [0, 252, 449]], '50':...</td>\n",
              "      <td>[[0, 252, 449], [253, 464, 450]]</td>\n",
              "      <td>[[253, 463, 450], [0, 252, 449]]</td>\n",
              "      <td>[449, 449, 449, 449, 449, 449, 449, 449, 449, ...</td>\n",
              "      <td>[449, 449, 449, 449, 449, 449, 449, 449, 449, ...</td>\n",
              "      <td>0.977612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>33351</td>\n",
              "      <td>Like many areas around the country, Washington...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 0, 'end_offset...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[0, 131, 450], [943, 1127, 450], [145,...</td>\n",
              "      <td>[[146, 394, 449], [397, 633, 450], [707, 942, ...</td>\n",
              "      <td>[[0, 131, 450], [943, 1127, 450], [145, 394, 4...</td>\n",
              "      <td>[450, 450, 450, 450, 450, 450, 450, 450, 450, ...</td>\n",
              "      <td>[450, 450, 450, 450, 450, 450, 450, 450, 450, ...</td>\n",
              "      <td>0.945719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>33352</td>\n",
              "      <td>Yes, the American military still uses bayonets...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 1755, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[1755, 1876, 450], [2063, 2162, 450], ...</td>\n",
              "      <td>[[87, 300, 450], [301, 437, 450], [689, 939, 4...</td>\n",
              "      <td>[[1755, 1876, 450], [2063, 2162, 450], [87, 30...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.879133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>33353</td>\n",
              "      <td>Washington Tax Break Urged for Firms That Keep...</td>\n",
              "      <td>[{'label': 450, 'start_offset': 722, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'53': [[722, 869, 450], [62, 354, 450], [506,...</td>\n",
              "      <td>[[507, 685, 449], [685, 721, 450], [869, 997, ...</td>\n",
              "      <td>[[722, 869, 450], [62, 354, 450], [506, 685, 4...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.925829</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... text-wise-alpha\n",
              "0   33334  ...        0.993452\n",
              "1   33335  ...        0.989754\n",
              "2   33336  ...        0.997436\n",
              "3   33337  ...        0.993184\n",
              "4   33338  ...        1.000000\n",
              "5   33339  ...        0.930985\n",
              "6   33340  ...        0.993963\n",
              "7   33341  ...        0.761209\n",
              "8   33342  ...        0.874042\n",
              "9   33343  ...        0.824292\n",
              "10  33344  ...        0.797818\n",
              "11  33345  ...        0.667596\n",
              "12  33346  ...        0.559220\n",
              "13  33347  ...        0.568560\n",
              "14  33348  ...        0.814418\n",
              "15  33349  ...        0.919943\n",
              "16  33350  ...        0.977612\n",
              "17  33351  ...        0.945719\n",
              "18  33352  ...        0.879133\n",
              "19  33353  ...        0.925829\n",
              "\n",
              "[20 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "statutory-coating",
        "outputId": "f74a2735-9840-4cc0-a877-7ffe4e0a5b7b"
      },
      "source": [
        "df1['text-wise-alpha'].fillna(1).mean()"
      ],
      "id": "statutory-coating",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8717970903733566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "handed-client",
        "outputId": "d95364b8-6b41-46b4-acb8-98c1ecd2d464"
      },
      "source": [
        "df1['text-wise-alpha'].dropna().mean()"
      ],
      "id": "handed-client",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8520735658154115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2MKXOZE7Qs-"
      },
      "source": [
        "Pour df2: horse-race"
      ],
      "id": "U2MKXOZE7Qs-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC6jJXke7S4v"
      },
      "source": [
        "df2['one_hot_51_2'] = df2.apply(lambda x: one_hot(x.annotateur_51_2, len(x.text)), axis=1)\r\n",
        "df2['one_hot_52_2'] = df2.apply(lambda x: one_hot(x.annotateur_52_2, len(x.text)), axis=1)"
      ],
      "id": "TC6jJXke7S4v",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAR3UtTn7Zhr",
        "outputId": "c624b565-2e9c-4a6f-f660-d9622b22e8cd"
      },
      "source": [
        "df2['text-wise-alpha'] = df2.apply(lambda x: krippendorff.alpha(reliability_data=[x.one_hot_51_2, x.one_hot_52_2], level_of_measurement='nominal'), axis=1)"
      ],
      "id": "XAR3UtTn7Zhr",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/krippendorff/krippendorff.py:273: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - (o * d).sum() / (e * d).sum()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5feenHU4B8EJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b02eed8-7c8e-4d4f-fde8-91a85ad1e3d7"
      },
      "source": [
        "df2.head(20)\r\n"
      ],
      "id": "5feenHU4B8EJ",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotations</th>\n",
              "      <th>meta</th>\n",
              "      <th>annotation_approver</th>\n",
              "      <th>simplified</th>\n",
              "      <th>annotateur_51_2</th>\n",
              "      <th>annotateur_52_2</th>\n",
              "      <th>one_hot_51_2</th>\n",
              "      <th>one_hot_52_2</th>\n",
              "      <th>text-wise-alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33364</td>\n",
              "      <td>HOLLAND, Pa. -- The Tea Party movement ignited...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 1039, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[1039, 1177, 452], [3978, 4040, 452], ...</td>\n",
              "      <td>[[1039, 1177, 452], [3978, 4040, 452], [4041, ...</td>\n",
              "      <td>[[1039, 1176, 452], [4041, 4095, 452], [3982, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.989159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33365</td>\n",
              "      <td>WASHINGTON, Oct. 12 -- President Bush delivere...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33366</td>\n",
              "      <td>AS oil and gasoline prices rise, political pre...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33367</td>\n",
              "      <td>Caucusgoers, uniquely, will declare their loya...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 297, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[297, 351, 452], [3484, 3637, 452], [5...</td>\n",
              "      <td>[[297, 351, 452], [3484, 3637, 452], [5564, 56...</td>\n",
              "      <td>[[1929, 1986, 452], [3484, 3638, 452], [5045, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.972769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33368</td>\n",
              "      <td>To the Editor:    Re ''Giuliani Says a Democra...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>33369</td>\n",
              "      <td>CARACAS, Venezuela, Nov. 29 -- Opponents of Pr...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>33370</td>\n",
              "      <td>WASHINGTON -- Leading Democrats on the House J...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33371</td>\n",
              "      <td>WASHINGTON -- George W. Bush has just placed h...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33372</td>\n",
              "      <td>WASHINGTON, March 26 -- The World Trade Organi...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33373</td>\n",
              "      <td>If you listened to the Republican candidates t...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 1404, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'52': [[1404, 1449, 452]], '51': [[1404, 1449...</td>\n",
              "      <td>[[1404, 1449, 452]]</td>\n",
              "      <td>[[1404, 1449, 452]]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33374</td>\n",
              "      <td>A bill banning abortions in Albuquerque after ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>33375</td>\n",
              "      <td>WASHINGTON -- Senator Barack Obama informed tw...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 4853, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[4853, 4867, 452], [2208, 2268, 452], ...</td>\n",
              "      <td>[[4853, 4867, 452], [2208, 2268, 452], [2478, ...</td>\n",
              "      <td>[[2207, 2267, 452], [2695, 2752, 452], [2478, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.993250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>33376</td>\n",
              "      <td>WASHINGTON, Feb. 23 -- The Energy Department s...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>33377</td>\n",
              "      <td>Most contributors to political campaigns are p...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>33378</td>\n",
              "      <td>WASHINGTON -- Government investigators said Fr...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 1542, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[1542, 1613, 452], [4420, 4502, 452]],...</td>\n",
              "      <td>[[1542, 1613, 452], [4420, 4502, 452]]</td>\n",
              "      <td>[[1542, 1612, 452], [4420, 4502, 452]]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.996630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>33379</td>\n",
              "      <td>WASHINGTON, Aug. 2 -- Federal officials moved ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>33380</td>\n",
              "      <td>Mayor Michael R. Bloomberg announced yesterday...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>33381</td>\n",
              "      <td>Like many areas around the country, Washington...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>33382</td>\n",
              "      <td>Yes, the American military still uses bayonets...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>33383</td>\n",
              "      <td>Washington Tax Break Urged for Firms That Keep...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... text-wise-alpha\n",
              "0   33364  ...        0.989159\n",
              "1   33365  ...             NaN\n",
              "2   33366  ...             NaN\n",
              "3   33367  ...        0.972769\n",
              "4   33368  ...             NaN\n",
              "5   33369  ...             NaN\n",
              "6   33370  ...             NaN\n",
              "7   33371  ...             NaN\n",
              "8   33372  ...             NaN\n",
              "9   33373  ...        1.000000\n",
              "10  33374  ...             NaN\n",
              "11  33375  ...        0.993250\n",
              "12  33376  ...             NaN\n",
              "13  33377  ...             NaN\n",
              "14  33378  ...        0.996630\n",
              "15  33379  ...             NaN\n",
              "16  33380  ...             NaN\n",
              "17  33381  ...             NaN\n",
              "18  33382  ...             NaN\n",
              "19  33383  ...             NaN\n",
              "\n",
              "[20 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E56wnSCo7aFm",
        "outputId": "8ab846f0-cd91-4bae-9b9e-4222c22830f2"
      },
      "source": [
        "df2['text-wise-alpha'].fillna(1).mean()"
      ],
      "id": "E56wnSCo7aFm",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9960887249498406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-IvNYU97dnS",
        "outputId": "55f60ff7-84e6-4d0f-a5bb-9fd0afcf2da7"
      },
      "source": [
        "df2['text-wise-alpha'].dropna().mean()"
      ],
      "id": "r-IvNYU97dnS",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9882661748495212"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pcr8T6l-VA4"
      },
      "source": [
        "Pour df3 : personnification\r\n"
      ],
      "id": "-Pcr8T6l-VA4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEb_7yXj-Y-T"
      },
      "source": [
        "df3['one_hot_51_3'] = df3.apply(lambda x: one_hot(x.annotateur_51_3, len(x.text)), axis=1)\r\n",
        "df3['one_hot_52_3'] = df3.apply(lambda x: one_hot(x.annotateur_52_3, len(x.text)), axis=1)"
      ],
      "id": "cEb_7yXj-Y-T",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiL1_bUp-klr",
        "outputId": "834cbeaf-e235-4f34-e260-6a5ed9a452f1"
      },
      "source": [
        "df3['text-wise-alpha'] = df3.apply(lambda x: krippendorff.alpha(reliability_data=[x.one_hot_51_3, x.one_hot_52_3], level_of_measurement='nominal'), axis=1)"
      ],
      "id": "eiL1_bUp-klr",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/krippendorff/krippendorff.py:273: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - (o * d).sum() / (e * d).sum()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhHFes0x-yUX",
        "outputId": "3f4713b7-afcf-47b2-ea38-6f9d8e5f3bdc"
      },
      "source": [
        "df3['text-wise-alpha'].fillna(1).mean()"
      ],
      "id": "IhHFes0x-yUX",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.971565270026758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vj26TRR-1lo",
        "outputId": "90ebcadf-061b-4c84-c4dc-3123c58cca81"
      },
      "source": [
        "df3['text-wise-alpha'].dropna().mean()"
      ],
      "id": "4vj26TRR-1lo",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9644565875334474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvAVdfSi_jdv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "3426c3c0-f4d1-49dd-a4f1-bb08a028550a"
      },
      "source": [
        "df2.head()"
      ],
      "id": "VvAVdfSi_jdv",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotations</th>\n",
              "      <th>meta</th>\n",
              "      <th>annotation_approver</th>\n",
              "      <th>simplified</th>\n",
              "      <th>annotateur_51_2</th>\n",
              "      <th>annotateur_52_2</th>\n",
              "      <th>one_hot_51_2</th>\n",
              "      <th>one_hot_52_2</th>\n",
              "      <th>text-wise-alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33364</td>\n",
              "      <td>HOLLAND, Pa. -- The Tea Party movement ignited...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 1039, 'end_off...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[1039, 1177, 452], [3978, 4040, 452], ...</td>\n",
              "      <td>[[1039, 1177, 452], [3978, 4040, 452], [4041, ...</td>\n",
              "      <td>[[1039, 1176, 452], [4041, 4095, 452], [3982, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.989159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33365</td>\n",
              "      <td>WASHINGTON, Oct. 12 -- President Bush delivere...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33366</td>\n",
              "      <td>AS oil and gasoline prices rise, political pre...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33367</td>\n",
              "      <td>Caucusgoers, uniquely, will declare their loya...</td>\n",
              "      <td>[{'label': 452, 'start_offset': 297, 'end_offs...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'51': [[297, 351, 452], [3484, 3637, 452], [5...</td>\n",
              "      <td>[[297, 351, 452], [3484, 3637, 452], [5564, 56...</td>\n",
              "      <td>[[1929, 1986, 452], [3484, 3638, 452], [5045, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0.972769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33368</td>\n",
              "      <td>To the Editor:    Re ''Giuliani Says a Democra...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... text-wise-alpha\n",
              "0  33364  ...        0.989159\n",
              "1  33365  ...             NaN\n",
              "2  33366  ...             NaN\n",
              "3  33367  ...        0.972769\n",
              "4  33368  ...             NaN\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kOAJWNa9x2q"
      },
      "source": [
        "##BERT\r\n",
        "\r\n",
        "A GPU can be added by going to the menu and selecting:\r\n",
        "\r\n",
        "Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)"
      ],
      "id": "2kOAJWNa9x2q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylqGo5Wx-H1O"
      },
      "source": [
        "Then run the following cell to confirm that the GPU is detected."
      ],
      "id": "ylqGo5Wx-H1O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hyqx2Ji-HI8",
        "outputId": "1f8c8d51-31a2-4714-ce2a-7894263d0e18"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "id": "3hyqx2Ji-HI8",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHkxX9AS-TqY"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device."
      ],
      "id": "BHkxX9AS-TqY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtndVzDN-UZF",
        "outputId": "59b1903b-e833-4824-9e85-33eaf3d76f67"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "id": "LtndVzDN-UZF",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdO-bIqW-iq7"
      },
      "source": [
        "## Installing the Hugging Face Library\r\n",
        "Next, let’s install the transformers package from Hugging Face which will give us a pytorch interface for working with BERT. "
      ],
      "id": "KdO-bIqW-iq7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QpfaC7l_CDE",
        "outputId": "e8c47438-0ffd-41d1-83c8-cb62a8a96916"
      },
      "source": [
        "!pip install transformers\r\n"
      ],
      "id": "7QpfaC7l_CDE",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=a5ea15319f78379db85676a1109f4e04302f5746eab911d4aca3a41d3f05263b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE-wburoJewi"
      },
      "source": [
        "## BERT Tokenizer\r\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\r\n",
        "\r\n",
        "The tokenization must be performed by the tokenizer included with BERT–the below cell will download this for us. We’ll be using the “uncased” version here."
      ],
      "id": "VE-wburoJewi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "e4181830af7b4ccbbee199a5a2f49e21",
            "9a948efefd714b72911ae060b7c95b4d",
            "108740df081d4b3590bb802a9324c7c2",
            "de1a5d1774b7479ba0f40463949ae795",
            "db29b464f267476d92923c0b11b0ec7c",
            "f059d8fb2f6f4f068f07d443d0067e9d",
            "655556acf7314708ad2c11c3c661bdf7",
            "dfda93994c9d4ff28bf258730cbb280a"
          ]
        },
        "id": "NXZF1nDhJio0",
        "outputId": "4ced441b-d79c-4f17-c736-22c2c033aafa"
      },
      "source": [
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# Load the BERT tokenizer.\r\n",
        "print('Loading BERT tokenizer...')\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "id": "NXZF1nDhJio0",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4181830af7b4ccbbee199a5a2f49e21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDeJ3cAJ1pl"
      },
      "source": [
        "When we actually convert all of our sentences, we’ll use the tokenize.encode function to handle both steps, rather than calling tokenize and convert_tokens_to_ids separately.\r\n"
      ],
      "id": "7zDeJ3cAJ1pl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8tMBYluaKoJ"
      },
      "source": [
        "import keras"
      ],
      "id": "F8tMBYluaKoJ",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GtyMwUDSovV"
      },
      "source": [
        " def encode(sentences):\r\n",
        "    # Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "    input_ids = []\r\n",
        "\r\n",
        "    # For every sentence...\r\n",
        "    for sent in sentences:\r\n",
        "        # `encode` will:\r\n",
        "        #   (1) Tokenize the sentence.\r\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "        #   (3) Append the `[SEP]` token to the end.\r\n",
        "        #   (4) Map tokens to their IDs.\r\n",
        "        encoded_sent = tokenizer.encode(\r\n",
        "                            sent,                      # Sentence to encode.\r\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
        "\r\n",
        "                            # This function also supports truncation and conversion\r\n",
        "                            # to pytorch tensors, but we need to do padding, so we\r\n",
        "                            # can't use these features :( .\r\n",
        "                            #max_length = 128,          # Truncate all sentences.\r\n",
        "                            #return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                       )\r\n",
        "\r\n",
        "        # Add the encoded sentence to the list.\r\n",
        "        input_ids.append(encoded_sent)\r\n",
        "\r\n",
        "    # Print sentence 0, now as a list of IDs.\r\n",
        "    print('Original: ', sentences[0])\r\n",
        "    print('Token IDs:', input_ids[0])\r\n",
        "    MAX_LEN = max([len(sen) for sen in input_ids])\r\n",
        "    print('Max sentence length: ', MAX_LEN)\r\n",
        "\r\n",
        "    print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\r\n",
        "\r\n",
        "    print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\r\n",
        "\r\n",
        "    # Pad our input tokens with value 0.\r\n",
        "    # \"post\" indicates that we want to pad and truncate at the end of the sequence,\r\n",
        "    # as opposed to the beginning.\r\n",
        "    input_ids = keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \r\n",
        "                              value=0, truncating=\"post\", padding=\"post\")\r\n",
        "\r\n",
        "    print('\\nDone.')\r\n",
        "    # Create attention masks\r\n",
        "    attention_masks = []\r\n",
        "\r\n",
        "    # For each sentence...\r\n",
        "    for sent in input_ids:\r\n",
        "\r\n",
        "        # Create the attention mask.\r\n",
        "        #   - If a token ID is 0, then it's padding, set the mask to 0.\r\n",
        "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\r\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]\r\n",
        "\r\n",
        "        # Store the attention mask for this sentence.\r\n",
        "        attention_masks.append(att_mask)\r\n",
        "    return input_ids, attention_masks"
      ],
      "id": "1GtyMwUDSovV",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2zYKyfwTS4g",
        "outputId": "49538e2b-a2ef-4f26-90f5-1d3621c8ba6d"
      },
      "source": [
        "input_ids, attention_masks = encode(df1.text.values)\r\n"
      ],
      "id": "f2zYKyfwTS4g",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  HOLLAND, Pa. -- The Tea Party movement ignited a year ago, fueled by anti-establishment anger. Now, Tea Party activists are trying to take over the establishment, ground up. Across the country, they are signing up to be Republican precinct leaders, a position so low-level that it often remains vacant, but which comes with the ability to vote for the party executives who endorse candidates, approve platforms and decide where the party spends money. TD A new group called the National Precinct Alliance says it has a coordinator in nearly every state to recruit Tea Party activists to fill the positions and has already swelled the number of like-minded members in Republican Party committees in Arizona and Nevada. Its mantra is this: take the precinct, take the state, take the party -- and force it to nominate conservatives rather than people they see as liberals in Republican clothing. Here, in a perennial battleground district outside Philadelphia, Tea Party activists are trying to strip the local committee of its influence in choosing the Republican nominee to run against Representative Patrick J. Murphy, a Democrat who won the seat in 2006 by about 1,500 votes. After the local party said it would stick to its custom of endorsing a candidate rather than holding an open primary, Tea Party groups decided to hold their own candidate forum where people could cast a ballot. If the party does not yield, the groups say they will host a debate, too. ''We kind of changed the rules,'' said Anastasia Przybylski, one of the organizers. The Tea Party movement, named the original tax revolt in 1773, might be better described as a diverse, rambunctious and Internet-connected network of groups, powered by grass-roots anxiety about the economy, bailouts and increasing government involvement in health care. At one extreme are militia members who have shown up at meetings wearing guns and suggesting that institutions like the Federal Reserve be eliminated. At the other are those like Ms. Przybylski, who describes herself as ''just a stay-at-home mom'' who became agitated about the federal stimulus package. And if the Democrats are big-government socialists, the Republicans, in the Tea Party mind, are enablers. In some recent polls, a hypothetical Tea Party wins more support than Democrats or Republicans, and the most anti-establishment Tea Party activists push to fight as a third party. But as the movement looks toward the midterm elections in November, a growing number of activists argue that the best way to translate anger into influence is to infiltrate the Republican establishment (Democrats being, for the average Tea Partier, beyond redemption). ''If you want to have revenge against the Republican Party for using you for so many years, the best way is to turn around and use the Republican Party to your advantage,'' said Eric Odom, a Tea Party activist in Chicago who recently started a political action committee, and on his blog urged Tea Partiers to stop complaining about the Republican Party and ''move in and take it over.'' Republican leaders have been trying to harness the Tea Party energy -- Michael Steele, the chairman of the Republican National Committee, recently called the Tea Parties ''a revelatory moment.'' ''It puts in stark relief where the American people are, how they feel and what they feel,'' Mr. Steele said. ''It's important for our party to appreciate and understand that so we can move toward it, and embrace it.'' Not all Republicans agree. Some say the party needs to broaden its reach, not cater to the fringe. The defining experience for many Tea Party groups was the special election in the 23rd Congressional District of New York in November, where party leaders chose a candidate whom conservatives viewed as a Republican in name only -- she supported same-sex marriage, abortion rights and the federal stimulus package. After activists flooded the district to support a conservative third-party candidate, the Republican dropped out and endorsed the Democrat, who won. Conservatives took the Republican retreat as a victory, but also saw the power of the party structure in deciding who the candidates will be. The rallying cry for more local involvement has been ''No more NY-23's.'' ''We don't want to see what happened in New York happen here,'' Ms. Przybylski said. The forum here drew nine candidates and a standing-room crowd in an auditorium built for 1,200. The questions organizers had drawn up for the candidates hinted at the issues important to so called Teapublicans. Will you pledge to vote against tax increases, even hidden taxes like those in health care reform? Should corporate executives who encourage illegal immigrants to stay because it is good for business be hauled off to jail? Do you believe manmade pollution is a significant contributor to global warming? (''I don't necessarily think there's been global warming,'' one candidate objected.) Each was asked to define the 10th Amendment, and to cite examples of where it ''might have been violated.'' ''It's my favorite amendment in the Constitution,'' exclaimed one candidate, Ira Hoffman. ''I can't believe it!'' The amendment declares that powers not granted to the federal government by the Constitution are reserved to the states or the people, and Tea Party activists hold that Congress has overstepped its bounds, particularly by legislating health care. So candidates were asked whether they would support efforts to nullify the health care bill? Finally, the moderator asked them if 2010 would be ''the year of the Tea Party.'' The candidates, and many in the audience, said it would, but only if the Tea Party advocates worked the system. ''I think we can do greater things working in a system that's established than we ever can being a bunch of anarchists,'' said Jennifer Turner Stefano, a vice president of a local Tea Party group who is contesting her local Republican committeeperson. Ms. Stefano, a stay-at-home mother and former television reporter, will have to get 10 signatures and put her name on the ballot to run. But the National Precinct Alliance estimates that about 60 percent of the roughly 150,000 local Republican committee seats are vacant and can be filled by essentially showing up. ''Even if you've got a slight majority, you just need maybe 26 states, then you can have your say in how the party goes,'' said Philip Glass, a former commercial mortgage banker in Cincinnati who is the national director of the precinct alliance. The precinct strategy, like the Tea Party movement itself, has spread via the Internet, on sites like Resistnet.com. A National Tea Party Convention in Nashville next month will feature seminars on how to take over starting at the precinct level. Advocates hold up the example of Las Vegas, where a group of about 30 people who had become friendly at Tea Party events last spring met to discuss how they could turn their crowds into political influence. One mentioned that there were about 500 open precinct committee positions in the local Republican Party. They recruited other activists and flooded the committee -- the Republican Party says it now has 780 committee people, up from about 300. In July, they approved a new executive committee, and Tony Warren, one of the organizers and a new precinct committeeman himself, said six out of seven executives are ''constitutional conservatives,'' in keeping with Tea Party ideology. With the bulk of Nevada's population in the Las Vegas area, the local committee was able to elect a conservative slate to the state party in December, including a state chairman who has said he wants to make the party ''safe'' for conservatives. As recently as last spring, Mr. Warren said, ''we didn't even know how the darn party worked.'' PHOTOS: The audience, top, at a forum held by Tea Party groups in Holland, Pa. At left, candidate hopefuls. Above, audience surveys are collected. (PHOTOGRAPHS BY JESSICA KOURKOUNIS FOR THE NEW YORK TIMES)(A19) NS\n",
            "Token IDs: [101, 7935, 1010, 6643, 1012, 1011, 1011, 1996, 5572, 2283, 2929, 22395, 1037, 2095, 3283, 1010, 17999, 2011, 3424, 1011, 5069, 4963, 1012, 2085, 1010, 5572, 2283, 10134, 2024, 2667, 2000, 2202, 2058, 1996, 5069, 1010, 2598, 2039, 1012, 2408, 1996, 2406, 1010, 2027, 2024, 6608, 2039, 2000, 2022, 3951, 18761, 4177, 1010, 1037, 2597, 2061, 2659, 1011, 2504, 2008, 2009, 2411, 3464, 10030, 1010, 2021, 2029, 3310, 2007, 1996, 3754, 2000, 3789, 2005, 1996, 2283, 12706, 2040, 2203, 5668, 2063, 5347, 1010, 14300, 7248, 1998, 5630, 2073, 1996, 2283, 15970, 2769, 1012, 14595, 1037, 2047, 2177, 2170, 1996, 2120, 18761, 4707, 2758, 2009, 2038, 1037, 10669, 1999, 3053, 2296, 2110, 2000, 13024, 5572, 2283, 10134, 2000, 6039, 1996, 4460, 1998, 2038, 2525, 21851, 1996, 2193, 1997, 2066, 1011, 13128, 2372, 1999, 3951, 2283, 9528, 1999, 5334, 1998, 7756, 1012, 2049, 25951, 2003, 2023, 1024, 2202, 1996, 18761, 1010, 2202, 1996, 2110, 1010, 2202, 1996, 2283, 1011, 1011, 1998, 2486, 2009, 2000, 23388, 11992, 2738, 2084, 2111, 2027, 2156, 2004, 13350, 1999, 3951, 5929, 1012, 2182, 1010, 1999, 1037, 14638, 2645, 16365, 2212, 2648, 4407, 1010, 5572, 2283, 10134, 2024, 2667, 2000, 6167, 1996, 2334, 2837, 1997, 2049, 3747, 1999, 10549, 1996, 3951, 9773, 2000, 2448, 2114, 4387, 4754, 1046, 1012, 7104, 1010, 1037, 7672, 2040, 2180, 1996, 2835, 1999, 2294, 2011, 2055, 1015, 1010, 3156, 4494, 1012, 2044, 1996, 2334, 2283, 2056, 2009, 2052, 6293, 2000, 2049, 7661, 1997, 2203, 5668, 2075, 1037, 4018, 2738, 2084, 3173, 2019, 2330, 3078, 1010, 5572, 2283, 2967, 2787, 2000, 2907, 2037, 2219, 4018, 7057, 2073, 2111, 2071, 3459, 1037, 10428, 1012, 2065, 1996, 2283, 2515, 2025, 10750, 1010, 1996, 2967, 2360, 2027, 2097, 3677, 1037, 5981, 1010, 2205, 1012, 1005, 1005, 2057, 2785, 1997, 2904, 1996, 3513, 1010, 1005, 1005, 2056, 19447, 10975, 9096, 3762, 4877, 3211, 1010, 2028, 1997, 1996, 18829, 1012, 1996, 5572, 2283, 2929, 1010, 2315, 1996, 2434, 4171, 10073, 1999, 19916, 1010, 2453, 2022, 2488, 2649, 2004, 1037, 7578, 1010, 8223, 8569, 12273, 20771, 1998, 4274, 1011, 4198, 2897, 1997, 2967, 1010, 6113, 2011, 5568, 1011, 6147, 10089, 2055, 1996, 4610, 1010, 15358, 12166, 1998, 4852, 2231, 6624, 1999, 2740, 2729, 1012, 2012, 2028, 6034, 2024, 8396, 2372, 2040, 2031, 3491, 2039, 2012, 6295, 4147, 4409, 1998, 9104, 2008, 4896, 2066, 1996, 2976, 3914, 2022, 5892, 1012, 2012, 1996, 2060, 2024, 2216, 2066, 5796, 1012, 10975, 9096, 3762, 4877, 3211, 1010, 2040, 5577, 2841, 2004, 1005, 1005, 2074, 1037, 2994, 1011, 2012, 1011, 2188, 3566, 1005, 1005, 2040, 2150, 21568, 2055, 1996, 2976, 19220, 7427, 1012, 1998, 2065, 1996, 8037, 2024, 2502, 1011, 2231, 21633, 1010, 1996, 10643, 1010, 1999, 1996, 5572, 2283, 2568, 1010, 2024, 9585, 2869, 1012, 1999, 2070, 3522, 14592, 1010, 1037, 25613, 5572, 2283, 5222, 2062, 2490, 2084, 8037, 2030, 10643, 1010, 1998, 1996, 2087, 3424, 1011, 5069, 5572, 2283, 10134, 5245, 2000, 2954, 2004, 1037, 2353, 2283, 1012, 2021, 2004, 1996, 2929, 3504, 2646, 1996, 3054, 3334, 2213, 3864, 1999, 2281, 1010, 1037, 3652, 2193, 1997, 10134, 7475, 2008, 1996, 2190, 2126, 2000, 17637, 4963, 2046, 3747, 2003, 2000, 29543, 1996, 3951, 5069, 1006, 8037, 2108, 1010, 2005, 1996, 2779, 5572, 2112, 3771, 1010, 3458, 18434, 1007, 1012, 1005, 1005, 2065, 2017, 2215, 2000, 2031, 7195, 2114, 1996, 3951, 2283, 2005, 2478, 2017, 2005, 2061, 2116, 2086, 1010, 1996, 2190, 2126, 2003, 2000, 2735, 2105, 1998, 2224, 1996, 3951, 2283, 2000, 2115, 5056, 1010, 1005, 1005, 2056, 4388, 1051, 9527, 1010, 1037, 5572, 2283, 7423, 1999, 3190, 2040, 3728, 2318, 1037, 2576, 2895, 2837, 1010, 1998, 2006, 2010, 9927, 9720, 5572, 2112, 10136, 2000, 2644, 17949, 2055, 1996, 3951, 2283, 1998, 1005, 1005, 2693, 1999, 1998, 2202, 2009, 2058, 1012, 1005, 1005, 3951, 4177, 2031, 2042, 2667, 2000, 17445, 1996, 5572, 2283, 2943, 1011, 1011, 2745, 12872, 1010, 1996, 3472, 1997, 1996, 3951, 2120, 2837, 1010, 3728, 2170, 1996, 5572, 4243, 1005, 1005, 1037, 7065, 10581, 7062, 2617, 1012, 1005, 1005, 1005, 1005, 2009, 8509, 1999, 9762, 4335, 2073, 1996, 2137, 2111, 2024, 1010, 2129, 2027, 2514, 1998, 2054, 2027, 2514, 1010, 1005, 1005, 2720, 1012, 12872, 2056, 1012, 1005, 1005, 2009, 1005, 1055, 2590, 2005, 2256, 2283, 2000, 9120, 1998, 3305, 2008, 2061, 2057, 2064, 2693, 2646, 2009, 1010, 1998, 9979, 2009, 1012, 1005, 1005, 2025, 2035, 10643, 5993, 1012, 2070, 2360, 1996, 2283, 3791, 2000, 5041, 2368, 2049, 3362, 1010, 2025, 23488, 2000, 1996, 13548, 1012, 1996, 12854, 3325, 2005, 2116, 5572, 2283, 2967, 2001, 1996, 2569, 2602, 1999, 1996, 13928, 7740, 2212, 1997, 2047, 2259, 1999, 2281, 1010, 2073, 2283, 4177, 4900, 1037, 4018, 3183, 11992, 7021, 2004, 1037, 3951, 1999, 2171, 2069, 1011, 1011, 2016, 3569, 2168, 1011, 3348, 3510, 1010, 11324, 2916, 1998, 1996, 2976, 19220, 7427, 1012, 2044, 10134, 10361, 1996, 2212, 2000, 2490, 1037, 4603, 2353, 1011, 2283, 4018, 1010, 1996, 3951, 3333, 2041, 1998, 11763, 1996, 7672, 1010, 2040, 2180, 1012, 11992, 2165, 1996, 3951, 7822, 2004, 1037, 3377, 1010, 2021, 2036, 2387, 1996, 2373, 1997, 1996, 2283, 3252, 1999, 10561, 2040, 1996, 5347, 2097, 2022, 1012, 1996, 8320, 2075, 5390, 2005, 2062, 2334, 6624, 2038, 2042, 1005, 1005, 2053, 2062, 6396, 1011, 2603, 1005, 1055, 1012, 1005, 1005, 1005, 1005, 2057, 2123, 1005, 1056, 2215, 2000, 2156, 2054, 3047, 1999, 2047, 2259, 4148, 2182, 1010, 1005, 1005, 5796, 1012, 10975, 9096, 3762, 4877, 3211, 2056, 1012, 1996, 7057, 2182, 3881, 3157, 5347, 1998, 1037, 3061, 1011, 2282, 4306, 1999, 2019, 11448, 2328, 2005, 1015, 1010, 3263, 1012, 1996, 3980, 18829, 2018, 4567, 2039, 2005, 1996, 5347, 21795, 2012, 1996, 3314, 2590, 2000, 2061, 2170, 5572, 14289, 16558, 5555, 3619, 1012, 2097, 2017, 16393, 2000, 3789, 2114, 4171, 7457, 1010, 2130, 5023, 7773, 2066, 2216, 1999, 2740, 2729, 5290, 1029, 2323, 5971, 12706, 2040, 8627, 6206, 7489, 2000, 2994, 2138, 2009, 2003, 2204, 2005, 2449, 2022, 13161, 2125, 2000, 7173, 1029, 2079, 2017, 2903, 2158, 21565, 10796, 2003, 1037, 3278, 12130, 2000, 3795, 12959, 1029, 1006, 1005, 1005, 1045, 2123, 1005, 1056, 9352, 2228, 2045, 1005, 1055, 2042, 3795, 12959, 1010, 1005, 1005, 2028, 4018, 15959, 1012, 1007, 2169, 2001, 2356, 2000, 9375, 1996, 6049, 7450, 1010, 1998, 2000, 21893, 4973, 1997, 2073, 2009, 1005, 1005, 2453, 2031, 2042, 14424, 1012, 1005, 1005, 1005, 1005, 2009, 1005, 1055, 2026, 5440, 7450, 1999, 1996, 4552, 1010, 1005, 1005, 12713, 2028, 4018, 1010, 11209, 15107, 1012, 1005, 1005, 1045, 2064, 1005, 1056, 2903, 2009, 999, 1005, 1005, 1996, 7450, 18806, 2008, 4204, 2025, 4379, 2000, 1996, 2976, 2231, 2011, 1996, 4552, 2024, 9235, 2000, 1996, 2163, 2030, 1996, 2111, 1010, 1998, 5572, 2283, 10134, 2907, 2008, 3519, 2038, 15849, 2618, 11469, 2049, 19202, 1010, 3391, 2011, 4190, 2483, 22248, 2740, 2729, 1012, 2061, 5347, 2020, 2356, 3251, 2027, 2052, 2490, 4073, 2000, 19701, 8757, 1996, 2740, 2729, 3021, 1029, 2633, 1010, 1996, 29420, 2356, 2068, 2065, 2230, 2052, 2022, 1005, 1005, 1996, 2095, 1997, 1996, 5572, 2283, 1012, 1005, 1005, 1996, 5347, 1010, 1998, 2116, 1999, 1996, 4378, 1010, 2056, 2009, 2052, 1010, 2021, 2069, 2065, 1996, 5572, 2283, 13010, 2499, 1996, 2291, 1012, 1005, 1005, 1045, 2228, 2057, 2064, 2079, 3618, 2477, 2551, 1999, 1037, 2291, 2008, 1005, 1055, 2511, 2084, 2057, 2412, 2064, 2108, 1037, 9129, 1997, 18448, 2015, 1010, 1005, 1005, 2056, 7673, 6769, 19618, 1010, 1037, 3580, 2343, 1997, 1037, 2334, 5572, 2283, 2177, 2040, 2003, 5049, 2075, 2014, 2334, 3951, 2837, 27576, 1012, 5796, 1012, 19618, 1010, 1037, 2994, 1011, 2012, 1011, 2188, 2388, 1998, 2280, 2547, 6398, 1010, 2097, 2031, 2000, 2131, 2184, 16442, 1998, 2404, 2014, 2171, 2006, 1996, 10428, 2000, 2448, 1012, 2021, 1996, 2120, 18761, 4707, 10035, 2008, 2055, 3438, 3867, 1997, 1996, 5560, 5018, 1010, 2199, 2334, 3951, 2837, 4272, 2024, 10030, 1998, 2064, 2022, 3561, 2011, 7687, 4760, 2039, 1012, 1005, 1005, 2130, 2065, 2017, 1005, 2310, 2288, 1037, 7263, 3484, 1010, 2017, 2074, 2342, 2672, 2656, 2163, 1010, 2059, 2017, 2064, 2031, 2115, 2360, 1999, 2129, 1996, 2283, 3632, 1010, 1005, 1005, 2056, 5170, 3221, 1010, 1037, 2280, 3293, 14344, 13448, 1999, 7797, 2040, 2003, 1996, 2120, 2472, 1997, 1996, 18761, 4707, 1012, 1996, 18761, 5656, 1010, 2066, 1996, 5572, 2283, 2929, 2993, 1010, 2038, 3659, 3081, 1996, 4274, 1010, 2006, 4573, 2066, 9507, 7159, 1012, 4012, 1012, 1037, 2120, 5572, 2283, 4680, 1999, 8423, 2279, 3204, 2097, 3444, 17239, 2006, 2129, 2000, 2202, 2058, 3225, 2012, 1996, 18761, 2504, 1012, 13010, 2907, 2039, 1996, 2742, 1997, 5869, 7136, 1010, 2073, 1037, 2177, 1997, 2055, 2382, 2111, 2040, 2018, 2468, 5379, 2012, 5572, 2283, 2824, 2197, 3500, 2777, 2000, 6848, 2129, 2027, 2071, 2735, 2037, 12783, 2046, 2576, 3747, 1012, 2028, 3855, 2008, 2045, 2020, 2055, 3156, 2330, 18761, 2837, 4460, 1999, 1996, 2334, 3951, 2283, 1012, 2027, 8733, 2060, 10134, 1998, 10361, 1996, 2837, 1011, 1011, 1996, 3951, 2283, 2758, 2009, 2085, 2038, 28601, 2837, 2111, 1010, 2039, 2013, 2055, 3998, 1012, 1999, 2251, 1010, 2027, 4844, 1037, 2047, 3237, 2837, 1010, 1998, 4116, 6031, 1010, 2028, 1997, 1996, 18829, 1998, 1037, 2047, 18761, 2837, 2386, 2370, 1010, 2056, 2416, 2041, 1997, 2698, 12706, 2024, 1005, 1005, 6543, 11992, 1010, 1005, 1005, 1999, 4363, 2007, 5572, 2283, 13165, 1012, 2007, 1996, 9625, 1997, 7756, 1005, 1055, 2313, 1999, 1996, 5869, 7136, 2181, 1010, 1996, 2334, 2837, 2001, 2583, 2000, 11322, 1037, 4603, 12796, 2000, 1996, 2110, 2283, 1999, 2285, 1010, 2164, 1037, 2110, 3472, 2040, 2038, 2056, 2002, 4122, 2000, 2191, 1996, 2283, 1005, 1005, 3647, 1005, 1005, 2005, 11992, 1012, 2004, 3728, 2004, 2197, 3500, 1010, 2720, 1012, 6031, 2056, 1010, 1005, 1005, 2057, 2134, 1005, 1056, 2130, 2113, 2129, 1996, 18243, 2078, 2283, 2499, 1012, 1005, 1005, 7760, 1024, 1996, 4378, 1010, 2327, 1010, 2012, 1037, 7057, 2218, 2011, 5572, 2283, 2967, 1999, 7935, 1010, 6643, 1012, 2012, 2187, 1010, 4018, 17772, 2015, 1012, 2682, 1010, 4378, 12265, 2024, 5067, 1012, 1006, 7008, 2011, 8201, 12849, 3126, 24861, 8977, 2005, 1996, 2047, 2259, 2335, 1007, 1006, 17350, 2683, 1007, 24978, 102]\n",
            "Max sentence length:  1873\n",
            "\n",
            "Padding/truncating all sentences to 1873 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RSM3F3Gpeka"
      },
      "source": [
        "labels = keras.preprocessing.sequence.pad_sequences(df1.one_hot_50_1.values, maxlen=345, dtype=\"long\", \r\n",
        "              value=0, truncating=\"post\", padding=\"post\")"
      ],
      "id": "3RSM3F3Gpeka",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFFYPJ9Lp9ks"
      },
      "source": [
        "##Training & Validation Split\r\n",
        "Divide up our training set to use 90% for training and 10% for validation."
      ],
      "id": "DFFYPJ9Lp9ks"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcqlLfk3p81I"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\r\n",
        "# training\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# Use 90% for training and 10% for validation.\r\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \r\n",
        "                                                            random_state=2021, test_size=0.1)\r\n",
        "# Do the same for the masks.\r\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\r\n",
        "                                             random_state=2021, test_size=0.1)"
      ],
      "id": "lcqlLfk3p81I",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEINlm8gsYX1"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \r\n",
        "# for our model.\r\n",
        "train_inputs = torch.tensor(train_inputs)\r\n",
        "validation_inputs = torch.tensor(validation_inputs)\r\n",
        "\r\n",
        "train_labels = torch.tensor(train_labels)\r\n",
        "validation_labels = torch.tensor(validation_labels)\r\n",
        "\r\n",
        "train_masks = torch.tensor(train_masks)\r\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "id": "kEINlm8gsYX1",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsGFp9n33qXi"
      },
      "source": [
        "##Train Our Classification Model\r\n",
        "Now that our input data is properly formatted, it’s time to fine tune the BERT model."
      ],
      "id": "LsGFp9n33qXi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrI1RrZj6dz-"
      },
      "source": [
        "###Training Loop\r\n",
        "Below is our training loop. There’s a lot going on, but fundamentally for each pass in our loop we have a training phase and a validation phase.\r\n",
        "\r\n",
        "Training:\r\n",
        "\r\n",
        "Unpack our data inputs and labels\r\n",
        "Load data onto the GPU for acceleration\r\n",
        "Clear out the gradients calculated in the previous pass.\r\n",
        "In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\r\n",
        "Forward pass (feed input data through the network)\r\n",
        "Backward pass (backpropagation)\r\n",
        "Tell the network to update parameters with optimizer.step()\r\n",
        "Track variables for monitoring progress\r\n",
        "Evalution:\r\n",
        "\r\n",
        "Unpack our data inputs and labels\r\n",
        "Load data onto the GPU for acceleration\r\n",
        "Forward pass (feed input data through the network)\r\n",
        "Compute loss on our validation data and track variables for monitoring progress\r\n",
        "Pytorch hides all of the detailed calculations from us, but we’ve commented the code to point out which of the above steps are happening on each line."
      ],
      "id": "nrI1RrZj6dz-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJz2xk5f6zFg"
      },
      "source": [
        "Define a helper function for calculating accuracy.\r\n"
      ],
      "id": "yJz2xk5f6zFg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2xiHYuh6r3W"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# Function to calculate the accuracy of our predictions vs labels\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "id": "H2xiHYuh6r3W",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb0sCxPk7AHG"
      },
      "source": [
        "Helper function for formatting elapsed times as hh:mm:ss"
      ],
      "id": "Kb0sCxPk7AHG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbmFexLa65yM"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n"
      ],
      "id": "SbmFexLa65yM",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWfXY8ol7G_N"
      },
      "source": [
        "We’re ready to kick off the training!"
      ],
      "id": "YWfXY8ol7G_N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMDqVRxsAE1u"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "from transformers import CamembertForTokenClassification, AdamW, CamembertConfig\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "\r\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\r\n",
        "# 16 or 32.\r\n",
        "\r\n",
        "def train(train_inputs, train_masks, train_labels, validation_inputs, validation_masks, validation_labels, epochs=1): \r\n",
        "    batch_size = 8\r\n",
        "\r\n",
        "    # Create the DataLoader for our training set.\r\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
        "    train_sampler = RandomSampler(train_data)\r\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "    # Create the DataLoader for our validation set.\r\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\r\n",
        "    validation_sampler = SequentialSampler(validation_data)\r\n",
        "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\r\n",
        "    \r\n",
        "\r\n",
        "    # Load BertForSequenceClassification, the pretrained BERT model with a single \r\n",
        "    # linear classification layer on top. \r\n",
        "    model = CamembertForTokenClassification.from_pretrained(\r\n",
        "        \"camembert-base\", # Use the 12-layer BERT model, with an uncased vocab.\r\n",
        "        num_labels = 4, # The number of output labels--2 for binary classification.\r\n",
        "                        # You can increase this for multi-class tasks.   \r\n",
        "        #output_attentions = False, # Whether the model returns attentions weights.\r\n",
        "        #output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
        "    )\r\n",
        "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
        "    # I believe the 'W' stands for 'Weight Decay fix\"\r\n",
        "    optimizer = AdamW(model.parameters(),\r\n",
        "                       lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
        "                      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                    )\r\n",
        "\r\n",
        "\r\n",
        "    # Total number of training steps is number of batches * number of epochs.\r\n",
        "    total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "    # Create the learning rate scheduler.\r\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\r\n",
        "                                                num_training_steps = total_steps)\r\n",
        "    \r\n",
        "    # Store the average loss after each epoch so we can plot them.\r\n",
        "    loss_values = []\r\n",
        "\r\n",
        "    # For each epoch...\r\n",
        "    for epoch_i in range(0, epochs):\r\n",
        "\r\n",
        "        # ========================================\r\n",
        "        #               Training\r\n",
        "        # ========================================\r\n",
        "\r\n",
        "        # Perform one full pass over the training set.\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "        print('Training...')\r\n",
        "\r\n",
        "        # Measure how long the training epoch takes.\r\n",
        "        t0 = time.time()\r\n",
        "\r\n",
        "        # Reset the total loss for this epoch.\r\n",
        "        total_loss = 0\r\n",
        "\r\n",
        "        # Put the model into training mode. Don't be mislead--the call to \r\n",
        "        # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "        # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        # For each batch of training data...\r\n",
        "        for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "            # Progress update every 40 batches.\r\n",
        "            if step % 40 == 0 and not step == 0:\r\n",
        "                # Calculate elapsed time in minutes.\r\n",
        "                elapsed = format_time(time.time() - t0)\r\n",
        "\r\n",
        "                # Report progress.\r\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "            # Unpack this training batch from our dataloader. \r\n",
        "            #\r\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "            # `to` method.\r\n",
        "            #\r\n",
        "            # `batch` contains three pytorch tensors:\r\n",
        "            #   [0]: input ids \r\n",
        "            #   [1]: attention masks\r\n",
        "            #   [2]: labels \r\n",
        "            b_input_ids = batch[0].to(device)\r\n",
        "            b_input_mask = batch[1].to(device)\r\n",
        "            b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "            # Always clear any previously calculated gradients before performing a\r\n",
        "            # backward pass. PyTorch doesn't do this automatically because \r\n",
        "            # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "            model.zero_grad()        \r\n",
        "\r\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "            # This will return the loss (rather than the model output) because we\r\n",
        "            # have provided the `labels`.\r\n",
        "            # The documentation for this `model` function is here: \r\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                        token_type_ids=None, \r\n",
        "                        attention_mask=b_input_mask,\r\n",
        "                        labels=b_labels)\r\n",
        "            # return outputs\r\n",
        "            # The call to `model` always returns a tuple, so we need to pull the \r\n",
        "            # loss value out of the tuple.\r\n",
        "            loss = outputs[0]\r\n",
        "            #scores = outputs[1]\r\n",
        "            #print(scores)\r\n",
        "\r\n",
        "            # Accumulate the training loss over all of the batches so that we can\r\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "            # single value; the `.item()` function just returns the Python value \r\n",
        "            # from the tensor.\r\n",
        "            total_loss += loss.item()\r\n",
        "            \r\n",
        "\r\n",
        "            # Perform a backward pass to calculate the gradients.\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "            # Clip the norm of the gradients to 1.0.\r\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "            # Update parameters and take a step using the computed gradient.\r\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "            # modified based on their gradients, the learning rate, etc.\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            # Update the learning rate.\r\n",
        "            scheduler.step()\r\n",
        "\r\n",
        "            # break\r\n",
        "        # Calculate the average loss over the training data.\r\n",
        "        avg_train_loss = total_loss / len(train_dataloader) \r\n",
        "\r\n",
        "         # Store the loss value for plotting the learning curve.\r\n",
        "        loss_values.append(avg_train_loss)\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "        # ========================================\r\n",
        "        #               Validation\r\n",
        "        # ========================================\r\n",
        "        # After the completion of each training epoch, measure our performance on\r\n",
        "        # our validation set.\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print(\"Running Validation...\")\r\n",
        "\r\n",
        "        t0 = time.time()\r\n",
        "\r\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "        # during evaluation.\r\n",
        "        model.eval()\r\n",
        "\r\n",
        "        # Tracking variables \r\n",
        "        eval_loss, eval_accuracy = 0, 0\r\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\r\n",
        "        true = []\r\n",
        "        pred = []\r\n",
        "        # Evaluate data for one epoch\r\n",
        "        for batch in validation_dataloader:\r\n",
        "\r\n",
        "            # Add batch to GPU\r\n",
        "            batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "            # Unpack the inputs from our dataloader\r\n",
        "            b_input_ids, b_input_mask, b_labels = batch\r\n",
        "\r\n",
        "            # Telling the model not to compute or store gradients, saving memory and\r\n",
        "            # speeding up validation\r\n",
        "            with torch.no_grad():        \r\n",
        "\r\n",
        "                # Forward pass, calculate logit predictions.\r\n",
        "                # This will return the logits rather than the loss because we have\r\n",
        "                # not provided labels.\r\n",
        "                # token_type_ids is the same as the \"segment ids\", which \r\n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "                # The documentation for this `model` function is here: \r\n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "\r\n",
        "                outputs = model(b_input_ids, \r\n",
        "                                token_type_ids=None, \r\n",
        "                                attention_mask=b_input_mask)\r\n",
        "\r\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "            # values prior to applying an activation function like the softmax.\r\n",
        "            logits = outputs[0]\r\n",
        "\r\n",
        "            # Move logits and labels to CPU\r\n",
        "            logits = logits.detach().cpu().numpy()\r\n",
        "            label_ids = b_labels.to('cpu').numpy()\r\n",
        "            mask = b_input_mask.detach().cpu()\r\n",
        "\r\n",
        "            #print(np.argmax(logits, axis=2))\r\n",
        "            #print(label_ids.shape)\r\n",
        "            lab = np.multiply(np.argmax(logits, axis=2), mask).reshape([-1])\r\n",
        "            #print(lab.shape)\r\n",
        "            #print(lab)\r\n",
        "            true_lab = np.multiply(label_ids, mask).reshape([-1])\r\n",
        "            \r\n",
        "            true = np.concatenate([true, true_lab])\r\n",
        "            pred = np.concatenate([pred, lab])\r\n",
        "\r\n",
        "            # Calculate the accuracy for this batch of test sentences.\r\n",
        "            #tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
        "\r\n",
        "            # Accumulate the total accuracy.\r\n",
        "            #eval_accuracy += tmp_eval_accuracy\r\n",
        "\r\n",
        "            # Track the number of batches\r\n",
        "            #nb_eval_steps += 1\r\n",
        "            # break\r\n",
        "\r\n",
        "        # Report the final accuracy for this validation run.\r\n",
        "        #print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "        print(classification_report(true, pred))\r\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Training complete!\")\r\n"
      ],
      "id": "IMDqVRxsAE1u",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586,
          "referenced_widgets": [
            "f0eb106059614cf8a8c7d46c54d0ec10",
            "2f8de44df6d34434a722af8988cb30a7",
            "ca6fdf2aee564a068052638f40b45d47",
            "223fdbe22cda41e6ad16f77cb9a444f4",
            "57aa8abf7aa24f22a2ffc3f9f526e20f",
            "6e3a908e14db405fbd2441f23fd2f0bf",
            "ad55337873d046d5ad73c888e2beaadd",
            "0bcf25dff25949ff843c92ad801838c1",
            "05f67cfcd1464ab5ba522e31b600b617",
            "2068460c33564621b441f57d8a448444",
            "9c6f998612de4f20b649e47a023bbdfc",
            "2324711fe1164aaa9b2eac8f38b54f5a",
            "64e0a9e5042645c6812194d6e5ded852",
            "38d89685271749689db2160f7809ecfc",
            "fa8c2d3d04c54dc5a8a7746462b5bbde",
            "60b75776b67e4b4891b82874dcddbddc"
          ]
        },
        "id": "z__qZepHA0Fk",
        "outputId": "225d0df8-4131-4638-f3d2-4465594aa3b2"
      },
      "source": [
        "outputs = train(train_inputs, train_masks, train_labels, validation_inputs, validation_masks, validation_labels, epochs=50)"
      ],
      "id": "z__qZepHA0Fk",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0eb106059614cf8a8c7d46c54d0ec10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=508.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05f67cfcd1464ab5ba522e31b600b617",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=445032417.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 50 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-8c03e8c76745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-7186524c0880>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_inputs, train_masks, train_labels, validation_inputs, validation_masks, validation_labels, epochs)\u001b[0m\n\u001b[1;32m    114\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                         labels=b_labels)\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# return outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# The call to `model` always returns a tuple, so we need to pull the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m         )\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m         )\n\u001b[1;32m    807\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    145\u001b[0m         return F.embedding(\n\u001b[1;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mciRjKng_F0h"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\r\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training & Validation Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.xticks([1, 2, 3, 4])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "id": "mciRjKng_F0h",
      "execution_count": null,
      "outputs": []
    }
  ]
}